{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook identifies which ml model should be used to train the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import *\n",
    "from scipy.stats import randint\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier, RUSBoostClassifier\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import tree\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "\n",
    "classifiers = [\n",
    "    LogisticRegression(),\n",
    "    GaussianNB(),\n",
    "    KNeighborsClassifier(3),\n",
    "    RandomForestClassifier(),\n",
    "    GradientBoostingClassifier(),\n",
    "    AdaBoostClassifier(),\n",
    "    LinearDiscriminantAnalysis(),\n",
    "    QuadraticDiscriminantAnalysis(),\n",
    "    BalancedRandomForestClassifier(),\n",
    "    RUSBoostClassifier()\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose which year you wish to test the model on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2021\n",
    "\n",
    "year = str(year)\n",
    "df = pd.read_csv('../csv_files/years/'+year+'.csv')\n",
    "df2 = pd.read_csv('../csv_files/random_sample/'+year+'.csv')\n",
    "artist_id = pd.read_csv('../csv_files/artist_id.csv')\n",
    "if 'df3' not in globals():\n",
    "    df3 = df.append(df2)\n",
    "else:\n",
    "    df3 = df3.append(df)\n",
    "    df3 = df3.append(df2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(artist_id['artist'])\n",
    "values = list(artist_id['artist_id'])\n",
    "artist_id = dict(zip(keys, values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist = []\n",
    "arr = df3['spotify_artist'].to_numpy().astype(str)\n",
    "arr = np.char.rstrip(arr, \"']\")\n",
    "arr = np.char.lstrip(arr, \"['\")\n",
    "arr = np.char.split(arr, \"', '\")\n",
    "for i, a_list in enumerate(arr):\n",
    "    if a_list[0] in artist_id:\n",
    "        artist.append(artist_id[a_list[0]])\n",
    "    else:\n",
    "        artist_id[a_list[0]] = len(artist_id) + 1\n",
    "        artist.append(artist_id[a_list[0]])\n",
    "df3['spotify_id'] = artist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy = df3.iloc[:,6:].to_numpy()\n",
    "X = Xy[:, :-2]\n",
    "y = Xy[:,-1]\n",
    "label = y.copy()\n",
    "label[y <= 0] = 0\n",
    "label[y > 0] = 1\n",
    "y = label\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code prints the prediction scores of each of the models used for comparison in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_score = 0\n",
    "max_test = \"\"\n",
    "for clf in classifiers:\n",
    "    print(\"=\"*30)\n",
    "    name = clf.__class__.__name__\n",
    "    print(name)\n",
    "    score = cross_validate(clf, X, y, scoring=['f1', 'precision', 'recall', 'accuracy', 'roc_auc'], n_jobs = -1, cv = 10)\n",
    "\n",
    "    print(\"Accuracy: {:.4%}\".format(np.mean(score['test_accuracy'])))\n",
    "    print(\"F1: {:.4%}\".format(np.mean(score['test_f1'])))\n",
    "    print(\"Recall: {:.4%}\".format(np.mean(score['test_recall'])))\n",
    "    print(\"Precision: {:.4%}\".format(np.mean(score['test_precision'])))\n",
    "    print(\"AUC: {:.4%}\".format(np.mean(score['test_roc_auc'])))\n",
    "\n",
    "    if np.mean(score['test_f1']) > max_score:\n",
    "        max_score = np.mean(score['test_roc_auc'])\n",
    "        max_test = clf.__class__.__name__\n",
    "    \n",
    "print(\"=\"*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates \n",
    "1. csv files that records the prediction scores of the models for each year.\n",
    "2. Also creates a csv file for feature importances.\n",
    "3. .pkl files for ml models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_df = pd.DataFrame(columns = ['accuracy', 'recall', 'precision'])\n",
    "f_importances = []\n",
    "for i in range(1980, 2022):\n",
    "    year = i\n",
    "\n",
    "\n",
    "    year = str(year)\n",
    "    df = pd.read_csv('../csv_files/years/'+year+'.csv')\n",
    "    df2 = pd.read_csv('../csv_files/random_sample/'+year+'.csv')\n",
    "    artist_id = pd.read_csv('../csv_files/artist_id.csv')\n",
    "    df3 = df.append(df2)\n",
    "\n",
    "    keys = list(artist_id['artist'])\n",
    "    values = list(artist_id['artist_id'])\n",
    "    artist_id = dict(zip(keys, values))\n",
    "\n",
    "    artist = []\n",
    "    arr = df3['spotify_artist'].to_numpy().astype(str)\n",
    "    arr = np.char.rstrip(arr, \"']\")\n",
    "    arr = np.char.lstrip(arr, \"['\")\n",
    "    arr = np.char.split(arr, \"', '\")\n",
    "    for i, a_list in enumerate(arr):\n",
    "        if a_list[0] in artist_id:\n",
    "            artist.append(artist_id[a_list[0]])\n",
    "        else:\n",
    "            artist_id[a_list[0]] = len(artist_id) + 1\n",
    "            artist.append(artist_id[a_list[0]])\n",
    "    df3['spotify_id'] = artist\n",
    "\n",
    "    Xy = df3.iloc[:,6:].to_numpy()\n",
    "    X = Xy[:, :-2]\n",
    "    y = Xy[:,-1]\n",
    "    label = y.copy()\n",
    "    label[y <= 0] = 0\n",
    "    label[y > 0] = 1\n",
    "    y = label\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "    clf = RandomForestClassifier()\n",
    "    clf.fit(X_train, y_train)\n",
    "    score = cross_validate(clf, X, y, scoring=['f1', 'precision', 'recall', 'accuracy', 'roc_auc'], n_jobs = -1, cv = 10)\n",
    "    scores = [score['test_accuracy'], score['test_recall'], score['test_precision']]\n",
    "    scores = np.mean(scores, axis = 1)\n",
    "    ml_df = ml_df.append(pd.DataFrame(list(scores.reshape(1,3)), columns = ['accuracy', 'recall', 'precision']))\n",
    "    f_importances.append(clf.feature_importances_)\n",
    "    pickle.dump(clf, open('..ml_models/' + year + '.pkl', 'wb'))\n",
    "\n",
    "df_importance = pd.DataFrame(f_importances, columns=['artist_id', 'danceability', 'energy', 'key' , 'loudness', 'mode', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', 'duration_ms'])\n",
    "df_importance.to_csv('../csv_files/f_importances.csv')\n",
    "ml_df.to_csv('../csv_files/ml_scores.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "703e297788ae86296449f676e0241f89b67adc670f29c8299b77ab5d7193a91f"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('music': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
